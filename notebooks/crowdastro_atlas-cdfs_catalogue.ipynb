{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowdastro ATLAS-CDFS Catalogue\n",
    "\n",
    "This notebook generates a catalogue of host galaxies for ATLAS-CDFS objects. This process proceeds as follows:\n",
    "\n",
    "1. Take a radio object.\n",
    "2. Find all nearby infrared objects.\n",
    "3. Classify all nearby infrared objects and predict the probability of a positive label.\n",
    "4. Select the infrared object with the highest probability. This is the host galaxy.\n",
    "\n",
    "This has some clear problems: What do we mean by \"nearby\"? What if we have two unrelated radio objects nearby each other? A model-based approach *Ã  la* Fan et al. (2015) may resolve this kind of issue, but as we are investigating a model-free approach, we leave this for future research. We take nearby to mean within a $1'$ radius, as this is the radius that Radio Galaxy Zoo volunteers see.\n",
    "\n",
    "In the code below, note that we internally represent ATLAS and SWIRE objects by IDs. These are arbitrary integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "We begin with some functions to perform the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports.\n",
    "from typing import List\n",
    "\n",
    "import astropy.io.ascii\n",
    "import astropy.table\n",
    "import h5py\n",
    "import numpy\n",
    "import sklearn.linear_model\n",
    "import sklearn.cross_validation\n",
    "\n",
    "# Globals.\n",
    "# This file stores the ATLAS-CDFS and SWIRE-CDFS catalogues.\n",
    "CROWDASTRO_PATH = '../data/crowdastro_swire.h5'\n",
    "# This file stores the training features and labels.\n",
    "TRAINING_PATH = '../data/training_swire.h5'\n",
    "# ATLAS catalogue.\n",
    "ATLAS_CATALOGUE_PATH = '../data/ATLASDR3_cmpcat_23July2015.csv'\n",
    "# Path to output catalogue to.\n",
    "OUTPUT_PATH = '../data/crowdastro_catalogue.dat'\n",
    "# Radius we should consider an object \"nearby\".\n",
    "NEARBY = 1 / 60  # 1 arcmin in degrees.\n",
    "# Size of an ATLAS image vector.\n",
    "IMAGE_SIZE = 200 * 200\n",
    "# Number of numeric features before the distance features.\n",
    "ATLAS_DIST_IDX = 2 + IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_host(probabilities: numpy.ndarray, atlas_id: int) -> int:\n",
    "    \"\"\"Finds the host galaxy associated with an ATLAS object.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    probabilities\n",
    "        (N,) array of predicted probabilities of SWIRE objects.\n",
    "    atlas_id\n",
    "        ID of the ATLAS object to find the host of.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        ID of predicted host galaxy.\n",
    "    \"\"\"\n",
    "    with h5py.File(CROWDASTRO_PATH, 'r') as cr, h5py.File(TRAINING_PATH, 'r') as tr:\n",
    "        # Get all nearby objects.\n",
    "        ir_distances = cr['/atlas/cdfs/numeric'][atlas_id, ATLAS_DIST_IDX:]\n",
    "        assert ir_distances.shape[0] == tr['features'].shape[0]\n",
    "        # Make a list of IDs of nearby objects.\n",
    "        nearby = sorted((ir_distances <= NEARBY).nonzero()[0])\n",
    "        # Find the best nearby candidate.\n",
    "        nearby_probabilities = probabilities[nearby]\n",
    "        # Select the highest probability object.\n",
    "        best_index = nearby_probabilities.argmax()\n",
    "        best_index = nearby[best_index]  # Convert back into an IR index.\n",
    "        return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier(indices: List[int]) -> sklearn.linear_model.LogisticRegression:\n",
    "    \"\"\"Trains a classifier.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    indices\n",
    "        List of infrared training indices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sklearn.linear_model.LogisticRegression\n",
    "        Trained logistic regression classifier.\n",
    "    \"\"\"\n",
    "    with h5py.File(TRAINING_PATH, 'r') as tr:\n",
    "        features = numpy.nan_to_num(tr['features'].value[indices])\n",
    "        labels = tr['labels'].value[indices]\n",
    "        lr = sklearn.linear_model.LogisticRegression(class_weight='balanced', penalty='l1')\n",
    "        lr.fit(features, labels)\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(classifier: sklearn.linear_model.LogisticRegression, indices: List[int]) -> numpy.ndarray:\n",
    "    \"\"\"Predicts probabilities for a set of IR objects.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    classifier\n",
    "        Trained classifier.\n",
    "    indices\n",
    "        List of IR indices to predict probability of.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        (N,) NumPy array of predicted probabilities.\n",
    "    \"\"\"\n",
    "    with h5py.File(TRAINING_PATH, 'r') as tr:\n",
    "        features = numpy.nan_to_num(tr['features'].value[indices])\n",
    "        return classifier.predict_proba(features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_predict(n_splits: int=10) -> numpy.ndarray:\n",
    "    \"\"\"Generates probabilities for IR objects.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Instances will be split according to ATLAS index, not IR index. This is\n",
    "    because there is overlap in IR objects' features, so we need to make sure\n",
    "    that this overlap is not present in the testing data.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    n_splits\n",
    "        Number of splits in cross-validation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        (N,) NumPy array of predictions.\n",
    "    \"\"\"\n",
    "    with h5py.File(CROWDASTRO_PATH, 'r') as cr:\n",
    "        # Get the number of ATLAS IDs.\n",
    "        n_atlas = cr['/atlas/cdfs/numeric'].shape[0]\n",
    "        # Get the number of SWIRE IDs.\n",
    "        n_swire = cr['/swire/cdfs/numeric'].shape[0]\n",
    "        # Allocate the array of predicted probabilities.\n",
    "        probabilities = numpy.zeros((n_swire,))\n",
    "        # Split into training/testing sets.\n",
    "        kf = sklearn.cross_validation.KFold(n_atlas, n_folds=n_splits)\n",
    "        # Train and predict.\n",
    "        for train_indices, test_indices in kf:\n",
    "            nearby_train = (cr['/atlas/cdfs/numeric'].value[train_indices, ATLAS_DIST_IDX:]\n",
    "                <= NEARBY).nonzero()[0]\n",
    "            nearby_test = (cr['/atlas/cdfs/numeric'].value[test_indices, ATLAS_DIST_IDX:]\n",
    "                <= NEARBY).nonzero()[0]\n",
    "            classifier = train_classifier(nearby_train)\n",
    "            fold_probs = predict(classifier, nearby_test)\n",
    "            probabilities[nearby_test] = fold_probs\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "In this section, we predict probabilities of SWIRE objects and use these probabilities to find the predicted host galaxies of ATLAS objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probabilities = train_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(CROWDASTRO_PATH, 'r') as cr:\n",
    "    n_atlas = cr['/atlas/cdfs/numeric'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hosts = [find_host(probabilities, i) for i in range(n_atlas)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the catalogue\n",
    "\n",
    "We now generate a catalogue matching each ATLAS object to a SWIRE host galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we need to get a list of the ATLAS and SWIRE object names.\n",
    "with h5py.File(CROWDASTRO_PATH, 'r') as cr:\n",
    "    atlas_ids = cr['/atlas/cdfs/string'].value\n",
    "    atlas_locs = cr['/atlas/cdfs/numeric'][:, :2]\n",
    "    # Convert ATLAS IDs into names.\n",
    "    atlas_catalogue = astropy.io.ascii.read(ATLAS_CATALOGUE_PATH)\n",
    "    id_to_name = {r['ID']: r['name'] for r in atlas_catalogue}\n",
    "    atlas_names = [id_to_name[id_.decode('ascii')] for zooniverse_id, id_ in atlas_ids]\n",
    "    \n",
    "    swire_names = [n.decode('ascii') for n in cr['/swire/cdfs/string']]\n",
    "    swire_locs = cr['/swire/cdfs/numeric'][:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can generate the catalogue.\n",
    "names = ('radio_object', 'infrared_host', 'ra', 'dec')\n",
    "table = astropy.table.Table(names=names, dtype=('S50', 'S50', 'float', 'float'))\n",
    "for atlas_index, atlas_name in enumerate(atlas_names):\n",
    "    host = hosts[atlas_index]\n",
    "    swire_name = swire_names[host]\n",
    "    ra, dec = swire_locs[host]\n",
    "    table.add_row((atlas_name, swire_name, ra, dec))\n",
    "\n",
    "astropy.io.ascii.write(table=table, output=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We will now compare this to the Norris et al. (2006) catalogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
