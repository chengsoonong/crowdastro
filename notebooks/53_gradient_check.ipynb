{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check\n",
    "\n",
    "I want to check if the analytic gradients I have match the gradient of the optimisation target. [#130](https://github.com/chengsoonong/crowdastro/issues/130)\n",
    "\n",
    "Of course, differentiation is\n",
    "\n",
    "$$Q'(x) = \\lim_{h \\to 0^+} \\frac{Q(x + h) - Q(x - h)}{2h}$$\n",
    "\n",
    "so we can just sample $h$ and compare the above to the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy, sys\n",
    "\n",
    "sys.path.insert(1, '..')\n",
    "import crowdastro.active_learning.passive_crowd as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x. (From Udacity)\"\"\"\n",
    "    return numpy.exp(x) / numpy.sum(numpy.exp(x), axis=0)\n",
    "\n",
    "def check(trials=10):\n",
    "    T = 4\n",
    "    D = 5\n",
    "    N = 20\n",
    "    \n",
    "    x = numpy.random.random(size=(N, D))\n",
    "    y = numpy.random.binomial(1, 0.5, size=(T, N))\n",
    "    \n",
    "    posteriors = numpy.random.random(size=(N,))\n",
    "    posteriors_0 = 1 - posteriors\n",
    "\n",
    "    params = numpy.random.normal(scale=0.5, size=(D + 1 + T * D + T,))\n",
    "    \n",
    "    print(pc.Q(params, D, T, N, posteriors, posteriors_0, x, y)[1])\n",
    "    \n",
    "    def Q(params):\n",
    "        a, b, w, g = pc.unpack(params, D, T)\n",
    "        return (posteriors.dot((numpy.log(pc.annotator_model(w, g, x, y, 1)) +\n",
    "                                numpy.log(pc.logistic_regression(a, b, x))).T) +\n",
    "                posteriors_0.dot((numpy.log(pc.annotator_model(w, g, x, y, 0)) +\n",
    "                                  numpy.log(1 - pc.logistic_regression(a, b, x))).T)\n",
    "                ).sum()\n",
    "\n",
    "    def approx_grad(params):\n",
    "        grads = []\n",
    "        for _ in range(100):\n",
    "            h = numpy.zeros(params.shape)\n",
    "            h[D + 1] = abs(numpy.random.normal(scale=numpy.linalg.norm(params) * 1e-10))\n",
    "            grads.append((pc.Q(params + h, D, T, N, posteriors, posteriors_0, x, y)[0] - pc.Q(params, D, T, N, posteriors, posteriors_0, x, y)[0]) / h[D + 1])\n",
    "        return numpy.mean(grads, axis=0)\n",
    "\n",
    "    def grad_b(params):\n",
    "        a, b, w, g = pc.unpack(params, D, T)\n",
    "        return T*(posteriors.dot(pc.logistic_regression(-a, -b, x)) +\n",
    "                  posteriors_0.dot(pc.logistic_regression(-a, -b, x) - 1))\n",
    "\n",
    "    def grad_a0(params):\n",
    "        a, b, w, g = pc.unpack(params, D, T)\n",
    "        return T*((x[:, 0] * posteriors).dot(pc.logistic_regression(-a, -b, x)) +\n",
    "                  (x[:, 0] * posteriors_0).dot(pc.logistic_regression(-a, -b, x) - 1))\n",
    "\n",
    "    def grad_Î³0(params):\n",
    "        a, b, w, g = pc.unpack(params, D, T)\n",
    "        return sum(posteriors[i] * (pc.logistic_regression(-w[0], -g[0], x[i]) - abs(y[0, i] - 1)) +\n",
    "                   posteriors_0[i] * (pc.logistic_regression(-w[0], -g[0], x[i]) - abs(y[0, i] - 0))\n",
    "                   for i in range(N))\n",
    "\n",
    "    def grad_w0(params):\n",
    "        a, b, w, g = pc.unpack(params, D, T)\n",
    "        ddw = numpy.zeros(w.shape)\n",
    "        for t in range(T):\n",
    "            ddw[t] += sum(x[i] * posteriors[i] * (pc.logistic_regression(-w[t], -g[t], x[i]) - abs(y[t, i] - 1)) +\n",
    "                          x[i] * posteriors_0[i] * (pc.logistic_regression(-w[t], -g[t], x[i]) - abs(y[t, i] - 0))\n",
    "                          for i in range(N))\n",
    "        return ddw\n",
    "    \n",
    "    return approx_grad(params), grad_w0(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.6965961   2.62526152  5.44163006  4.55768567  0.75906086  7.94502106\n",
      "  2.38303711  1.74267135  1.93973346  1.70254263  2.0052662  -2.4560358\n",
      " -1.9239815  -1.90423456 -1.10316555 -1.87786511 -0.33029406 -1.11499397\n",
      " -1.40680896 -0.91570391 -0.86608039  0.07425844 -0.04972941 -0.15018714\n",
      " -0.43148124  0.43932629  3.72774373 -3.58546661 -1.84149477  0.25027849]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.384527463426005,\n",
       " array([[-2.38303711, -1.74267135, -1.93973346, -1.70254263, -2.0052662 ],\n",
       "        [ 2.4560358 ,  1.9239815 ,  1.90423456,  1.10316555,  1.87786511],\n",
       "        [ 0.33029406,  1.11499397,  1.40680896,  0.91570391,  0.86608039],\n",
       "        [-0.07425844,  0.04972941,  0.15018714,  0.43148124, -0.43932629]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
