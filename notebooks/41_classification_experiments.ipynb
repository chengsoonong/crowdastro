{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATLAS Classification Experiments\n",
    "\n",
    "In this notebook, I want to answer some questions. The main tool for investigation will be a set of uncertainty measures generated with $k$-fold committee classification. Each ATLAS object will be assigned an uncertainty.\n",
    "\n",
    "- What is the distribution of these uncertainties?\n",
    "- Are the very certain objects interesting?\n",
    "- Are the very uncertain objects interesting?\n",
    "- If we compute a similar uncertainty for each IR object, is there a relationship between the uncertainty of the IR objects and the radio subjects they appear near?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: A classifier for the Radio Galaxy Zoo\n",
    "\n",
    "In this section, I'll build a classifier class to make the problem a little easier to grasp. A `RGZClassifier` will have access to all IR objects in the crowdastro database. It will have a `train` method which takes an IR label training set and a set of IR indices as arguments and trains the classifier, and a `predict` method which takes an ATLAS subject vector and returns an IR index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "ARCMIN = 1 / 60\n",
    "CROWDASTRO_H5_PATH = '../crowdastro.h5'\n",
    "IMAGE_SIZE = 200 * 200\n",
    "TRAINING_H5_PATH = '../training.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RGZClassifier(object):\n",
    "    \n",
    "    def __init__(self, ir_features, n_astro):\n",
    "        self.ir_features = ir_features\n",
    "        self.n_astro = n_astro\n",
    "        self._classifier = sklearn.linear_model.LogisticRegression(class_weight='balanced')\n",
    "        self._astro_transformer = sklearn.pipeline.Pipeline([\n",
    "            ('normalise', sklearn.preprocessing.Normalizer()),\n",
    "            ('scale', sklearn.preprocessing.StandardScaler()),\n",
    "        ])\n",
    "        self._image_transformer = sklearn.pipeline.Pipeline([\n",
    "            ('normalise', sklearn.preprocessing.Normalizer()),\n",
    "        ])\n",
    "    \n",
    "    def _fit_transformers(self, ir_indices):\n",
    "        self._astro_transformer.fit(self.ir_features[ir_indices, :self.n_astro])\n",
    "        self._image_transformer.fit(self.ir_features[ir_indices, self.n_astro:])\n",
    "    \n",
    "    def _transform(self, features):\n",
    "        return numpy.hstack([\n",
    "            self._astro_transformer.transform(features[:, :self.n_astro]),\n",
    "            self._image_transformer.transform(features[:, self.n_astro:]),\n",
    "        ])\n",
    "    \n",
    "    def train(self, ir_indices, ir_labels):\n",
    "        self._fit_transformers(ir_indices)\n",
    "        ir_features = self._transform(self.ir_features[ir_indices])\n",
    "        self._classifier.fit(ir_features, ir_labels)\n",
    "    \n",
    "    def predict(self, atlas_vector):\n",
    "        # Split the ATLAS vector into its components.\n",
    "        position = atlas_vector[:2]\n",
    "        image = atlas_vector[2:2 + IMAGE_SIZE]\n",
    "        distances = atlas_vector[2 + IMAGE_SIZE:]\n",
    "        # Get nearby IR objects and their features.\n",
    "        nearby_indices = (distances < ARCMIN).nonzero()[0]\n",
    "        ir_features = self._transform(self.ir_features[nearby_indices])\n",
    "        # Find how likely each object is to be the host galaxy.\n",
    "        probabilities = self._classifier.predict_proba(ir_features)[:, 1]\n",
    "        # Return the index of the most likely host galaxy.\n",
    "        return nearby_indices[probabilities.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helper functions for testing.\n",
    "def get_groundtruth_labels(atlas_vector, ir_labels):\n",
    "    distances = atlas_vector[2 + IMAGE_SIZE:]\n",
    "    nearby_indices = (distances < ARCMIN).nonzero()[0]\n",
    "    nearby_labels = ir_labels[nearby_indices]\n",
    "    groundtruth = nearby_labels.nonzero()[0]\n",
    "    return nearby_indices[groundtruth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.00%\n"
     ]
    }
   ],
   "source": [
    "# Let's test it out.\n",
    "with h5py.File(TRAINING_H5_PATH, 'r') as training_h5, \\\n",
    "     h5py.File(CROWDASTRO_H5_PATH, 'r') as crowdastro_h5:\n",
    "    ir_features = training_h5['features'].value\n",
    "    ir_labels = training_h5['labels'].value\n",
    "    ir_train_indices = training_h5['is_ir_train'].value.nonzero()[0]\n",
    "\n",
    "    classifier = RGZClassifier(ir_features, 5)\n",
    "    classifier.train(ir_train_indices, ir_labels[ir_train_indices])\n",
    "\n",
    "    atlas_vectors = crowdastro_h5['/atlas/cdfs/numeric']\n",
    "    atlas_test_indices = training_h5['is_atlas_test'].value.nonzero()[0]\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for atlas_index in atlas_test_indices:\n",
    "        atlas_vector = atlas_vectors[atlas_index]\n",
    "        groundtruths = get_groundtruth_labels(atlas_vector, ir_labels)\n",
    "        prediction = classifier.predict(atlas_vector)\n",
    "        n_correct += prediction in groundtruths\n",
    "        n_total += 1\n",
    "    \n",
    "    print('{:.02%}'.format(n_correct/n_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty by committee\n",
    "\n",
    "In this section, I'll make a committee of 10 classifiers which will learn on different subsets of the training data. I'll use 5-fold cross-validation to generate 10 classifications for each ATLAS object, and the percentage of agreement with the majority will form an uncertainty estimate for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
