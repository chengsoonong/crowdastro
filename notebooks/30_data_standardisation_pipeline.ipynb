{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Standardisation Pipeline <small>(version 0.1.0)</small>\n",
    "\n",
    "In this notebook, I detail the process of importing data into `crowdastro`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data sources\n",
    "\n",
    "The input data sources are:\n",
    "\n",
    "- Radio Galaxy Zoo\n",
    "    - Subjects (`radio_subjects.json`)\n",
    "    - Classifications (`radio_classifications.json`)\n",
    "- ATLAS\n",
    "    - Catalogue (`ATLASDR3_cmpcat_23July2015.dat`)\n",
    "    - FITS images of the radio sky (`cdfs` & `elais`)\n",
    "- SWIRE\n",
    "    - SWIRE Catalogue (`SWIRE3_CDFS_cat_IRAC24_21Dec05.tbl`)\n",
    "    - FITS images of the infrared sky (`cdfs` & `elais`)\n",
    "\n",
    "Paths to these should be specified in `crowdastro.json`. Radio Galaxy Zoo data should be imported into a database in MongoDB, specified by `radio_galaxy_zoo_db` in `crowdastro.json`. Following is an example `crowdastro.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"data_sources\": {\n",
    "        \"atlas_catalogue\": \"data/ATLASDR3_cmpcat_23July2015.dat\",\n",
    "        \"cdfs_fits\": \"data/cdfs\",\n",
    "        \"elais_s1_fits\": \"data/elais\",\n",
    "        \"radio_galaxy_zoo_db\": \"radio\",\n",
    "        \"swire_catalogue\": \"data/SWIRE3_CDFS_cat_IRAC24_21Dec05.tbl\"\n",
    "    },\n",
    "\n",
    "    \"mongo\": {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 27017\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data format\n",
    "\n",
    "The input data is converted into the output data by this pipeline. There are two output files. The files are `crowdastro.h5` and `crowdastro.csv`.\n",
    "\n",
    "The `.h5` files contain numeric data, including all FITS images (both radio and infrared), classifications, and subject metadata. The structure is as follows, with datasets italicised:\n",
    "\n",
    "- `/`\n",
    "    - `atlas`\n",
    "        - `cdfs`\n",
    "            - *`images_2x2`*\n",
    "            - *`images_5x5`*\n",
    "            - *`classification_positions`*\n",
    "            - *`classification_combinations`*\n",
    "            - *`positions`*\n",
    "            - *`training_indices`*\n",
    "            - *`validation_indices`*\n",
    "            - *`testing_indices`*\n",
    "        - `elais-s1`\n",
    "            - *`images_2x2`*\n",
    "            - *`images_5x5`*\n",
    "            - *`classification_positions`*\n",
    "            - *`classification_combinations`*\n",
    "            - *`positions`*\n",
    "            - *`training_indices`*\n",
    "            - *`validation_indices`*\n",
    "            - *`testing_indices`*\n",
    "    - `swire`\n",
    "        - `cdfs`\n",
    "            - *`images_2x2`*\n",
    "            - *`images_5x5`*\n",
    "            - *`catalogue`*\n",
    "        - `elais-s1`\n",
    "            - *`images_2x2`*\n",
    "            - *`images_5x5`*\n",
    "            - *`catalogue`*\n",
    "\n",
    "I'm only using ATLAS and SWIRE for now, but this is easily generalised to FIRST and WISE or EMU and MIGHTEE.\n",
    "\n",
    "*`training_indices`*, *`testing_indices`*, and *`validation_indices`* are non-overlapping subsets of the indices of the relevant data set. This is used to consistently partition into training/testing/validation data sets. The partitioning ratio is specified in `crowdastro.json` as `test_size` and `validation_size`. Note that `test_size` is first used to partition the data, and then `validation_size` is used to partition *the remaining data*.\n",
    "\n",
    "The `.csv` files contain textual data such as Zooniverse IDs. They are pretty much lookup tables; the reason for using CSV instead of HDF5 tables is partly for human readability and partly because dealing with textual data in HDF5 is unpleasant. The columns (examples parenthesised) are:\n",
    "\n",
    "- `index` (1)\n",
    "- `survey` (atlas)\n",
    "- `field` (cdfs)\n",
    "- `zooniverse_id` (ARG0003r18)\n",
    "- `name` (ATLAS3_J033403.6-282423C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing ATLAS data\n",
    "\n",
    "The ATLAS dataset consists of images of the radio sky in two fields, CDFS and ELAIS-S1, as well as a catalogue of objects in these fields. Unfortunately, the identifiers of the images and the identifiers in the catalogue are *different*, so we cannot rely on them matching (in fact, they don't match *at all*). The ATLAS catalogue identifiers correspond to nothing I've found so far, but the CDFS/ELAIS-S1 identifiers correspond to the image names, the Galaxy Zoo: Radio Talk survey IDs, and the radio subjects' `source` field in the Radio Galaxy Zoo dataset. There's a catalogue associated with the images, but it doesn't include the ATLAS names.\n",
    "\n",
    "Each object in the dataset is called a \"radio component\" and the $2' \\times 2'$ patch of radio sky centred on each component is called a \"radio subject\". There are $2460$ CDFS components and $1935$ ELAIS-S1 components &mdash; but the ELAIS-S1 components do not appear in Radio Galaxy Zoo at all.\n",
    "\n",
    "The origin of each image is in the *top left* corner. Each image is $201$ pixels in width and height.\n",
    "\n",
    "The RA/DEC of each component is stored in the Radio Galaxy Zoo dataset.\n",
    "\n",
    "Each component is sorted by Zooniverse ID in all output data. The images are stored in `/atlas/{cdfs,elais-s1}/images`. The RA/DEC of the components are stored in `/atlas/{cdfs,elais-s1}/positions` in decimal degrees. The names and Zooniverse IDs of each component are stored directly in the CSV along with the index they have in the HDF5 file.\n",
    "\n",
    "ATLAS components are only imported if they have a corresponding Zooniverse ID (i.e., if they are in the Radio Galaxy Zoo dataset), and if they are within a threshold radius from a Zooniverse object. The threshold is set as `radio_location_threshold` in `crowdastro.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing SWIRE data\n",
    "\n",
    "The SWIRE dataset consists of images of the infrared sky in two fields, CDFS and ELAIS-S1, as well as a catalogue of objects in these fields. They have no associated Zooniverse object, so we're just going to import them all.\n",
    "\n",
    "There are images from SWIRE, but these are actually associated with ATLAS objects, so they are imported alongside ATLAS data. We thus only have to import the catalogue into `/swire/{cdfs,elais-s1}/catalogue` and into the CSV. All coordinates are in RA/DEC in decimal degrees. Each object is sorted by SWIRE name. The index in the CSV corresponds to the index in the HDF5 catalogue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Radio Galaxy Zoo classifications\n",
    "\n",
    "Radio Galaxy Zoo (RGZ) classifications consist of a number of annotations of a subject. We only care about the combination of radio contours selected by a volunteer, and the corresponding pixel locations, as well as the Zooniverse ID associated with the subject.\n",
    "\n",
    "For each subject in the ATLAS dataset, parse all classifications. All valid classifications are then stored in `/atlas/{cdfs,elais-s1}/{classification_positions,classification_combinations}` with the position of the classification and the corresponding radio combination (and corresponding *full* radio combination) respectively. These also contain the ATLAS index they are associated with, and are sorted by this index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
