{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redundancy and KDE/PG-means\n",
    "\n",
    "Is the redundancy high enough on the Radio Galaxy Zoo website? Do PG-means or KDE handle low redundancy cases differently? How often does taking the mean give an accurate or useful result (based on Norris labels)? If PG-means and KDE disagree, what do the disagreements look like? If there are similar results for both, which is faster?\n",
    "\n",
    "*Bonus question:* Can we model the redundancy needed for a given radio object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import sklearn\n",
    "\n",
    "sys.path.insert(1, '..')\n",
    "# Needs MongoDB running.\n",
    "import crowdastro.rgz_analysis.consensus as consensus, crowdastro.rgz_data as data\n",
    "from crowdastro.consensuses import pg_means\n",
    "\n",
    "ARCMIN = 1 / 60\n",
    "CROWDASTRO_H5_PATH = '../data/crowdastro.h5'\n",
    "IMAGE_SIZE = 200 * 200\n",
    "NORRIS_DAT_PATH = '../data/norris_2006_atlas_classifications_ra_dec_only.dat'\n",
    "TRAINING_H5_PATH = '../data/training.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How often does KDE fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_kde_success = 0\n",
    "n_kde_failure_not_caught = 0\n",
    "n_kde_failure_caught = 0\n",
    "n_kde_total = 0\n",
    "\n",
    "for subject in data.get_all_subjects(survey='atlas', field='cdfs').limit(2000):\n",
    "    c = consensus.consensus(subject['zooniverse_id'], None)\n",
    "    for answer in c['answer'].values():\n",
    "        if 'ir_peak' in answer and answer['peak_data']['npeaks'] < 10:\n",
    "            n_kde_success += 1\n",
    "        elif 'ir_peak' in answer:\n",
    "            n_kde_failure_not_caught += 1\n",
    "        elif 'ir' in answer:\n",
    "            n_kde_failure_caught += 1\n",
    "        n_kde_total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDE succeeded: 90.42%\n",
      "KDE failed (caught, mean used): 8.88%\n",
      "KDE failed (not caught): 0.68%\n"
     ]
    }
   ],
   "source": [
    "print('KDE succeeded: {:.02%}'.format(n_kde_success / n_kde_total))\n",
    "print('KDE failed (caught, mean used): {:.02%}'.format(n_kde_failure_caught / n_kde_total))\n",
    "print('KDE failed (not caught): {:.02%}'.format(n_kde_failure_not_caught / n_kde_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How often does PG-means fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_pg_success = 0\n",
    "n_pg_failure = 0\n",
    "n_pg_total = 0\n",
    "\n",
    "# Taken mostly from crowdastro.consensuses.find_consensuses.\n",
    "with h5py.File(CROWDASTRO_H5_PATH, 'r') as f_h5:\n",
    "    class_positions = f_h5['/atlas/cdfs/classification_positions']\n",
    "    class_combinations = f_h5['/atlas/cdfs/classification_combinations']\n",
    "    ir_coords = f_h5['/{}/cdfs/numeric'.format('wise')][:, :2]\n",
    "    pos_groups = itertools.groupby(class_positions, key=lambda z: z[0])\n",
    "    com_groups = itertools.groupby(class_combinations, key=lambda z: z['index'])\n",
    "    for (i, pos_group), (j, com_group) in zip(pos_groups, com_groups):\n",
    "        assert i == j\n",
    "        com_group = list(com_group)\n",
    "        pos_group = list(pos_group)\n",
    "        total_classifications = 0\n",
    "        radio_counts = {}\n",
    "        for _, full_com, _ in com_group:\n",
    "            count = radio_counts.get(full_com, 0)\n",
    "            count += 1 / (full_com.count(b'|') + 1)\n",
    "            radio_counts[full_com] = count\n",
    "            total_classifications += 1 / (full_com.count(b'|') + 1)\n",
    "        radio_consensus = max(radio_counts, key=radio_counts.get)\n",
    "        for radio_signature in radio_consensus.split(b'|'):\n",
    "            n_pg_total += 1\n",
    "            percentage_consensus = (radio_counts[radio_consensus] /\n",
    "                                    total_classifications)\n",
    "            locations = []\n",
    "            for (_, x, y), (_, full, radio) in zip(pos_group, com_group):\n",
    "                if full == radio_consensus and radio == radio_signature:\n",
    "                    locations.append((x, y))\n",
    "            locations = numpy.array(locations)\n",
    "            locations = locations[~numpy.all(numpy.isnan(locations), axis=1)]\n",
    "            (x, y), success = pg_means(locations)\n",
    "            \n",
    "            if not success:\n",
    "                n_pg_failure += 1\n",
    "            else:\n",
    "                n_pg_success += 1\n",
    "\n",
    "            if numpy.isnan(x) or numpy.isnan(y):\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG-means succeeded: 13.01%\n",
      "PG-means failed (caught): 86.99%\n"
     ]
    }
   ],
   "source": [
    "print('PG-means succeeded: {:.02%}'.format(n_pg_success / n_pg_total))\n",
    "print('PG-means failed (caught): {:.02%}'.format(n_pg_failure / n_pg_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a surprisingly high number of failures. Most likely, the problem is a parameter problem, but instead of fiddling with parameters, let's just use a simpler method.\n",
    "\n",
    "## Lowest BIC GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowest_bic(locations):\n",
    "    min_bic = float('inf')\n",
    "    min_gmm = None\n",
    "    for k in range(1, 5):  # Assume no more than 5 candidate objects. Probably reasonable given ~20 clicks max.\n",
    "        gmm = sklearn.mixture.GMM(n_components=k, covariance_type='full')\n",
    "        try:\n",
    "            gmm.fit(locations)\n",
    "        except ValueError:\n",
    "            break\n",
    "        bic = gmm.bic(locations)\n",
    "        if bic < min_bic:\n",
    "            min_bic = bic\n",
    "            min_gmm = gmm\n",
    "    \n",
    "    if not min_gmm:\n",
    "        return locations.mean(axis=0), False, 'mean'\n",
    "    \n",
    "    if sum(w == max(min_gmm.weights_) for w in min_gmm.weights_) > 1:\n",
    "        success = False\n",
    "        reason = 'low_redundancy'\n",
    "    else:\n",
    "        success = True\n",
    "        reason = ''\n",
    "    \n",
    "    return min_gmm.means_[min_gmm.weights_.argmax()], success, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_bic_success = 0\n",
    "n_bic_failure_mean = 0\n",
    "n_bic_failure_tie = 0\n",
    "n_bic_total = 0\n",
    "\n",
    "# Taken mostly from crowdastro.consensuses.find_consensuses.\n",
    "with h5py.File(CROWDASTRO_H5_PATH, 'r') as f_h5:\n",
    "    class_positions = f_h5['/atlas/cdfs/classification_positions']\n",
    "    class_combinations = f_h5['/atlas/cdfs/classification_combinations']\n",
    "    ir_coords = f_h5['/{}/cdfs/numeric'.format('wise')][:, :2]\n",
    "    pos_groups = itertools.groupby(class_positions, key=lambda z: z[0])\n",
    "    com_groups = itertools.groupby(class_combinations, key=lambda z: z['index'])\n",
    "    for (i, pos_group), (j, com_group) in zip(pos_groups, com_groups):\n",
    "        assert i == j\n",
    "        com_group = list(com_group)\n",
    "        pos_group = list(pos_group)\n",
    "        total_classifications = 0\n",
    "        radio_counts = {}\n",
    "        for _, full_com, _ in com_group:\n",
    "            count = radio_counts.get(full_com, 0)\n",
    "            count += 1 / (full_com.count(b'|') + 1)\n",
    "            radio_counts[full_com] = count\n",
    "            total_classifications += 1 / (full_com.count(b'|') + 1)\n",
    "        radio_consensus = max(radio_counts, key=radio_counts.get)\n",
    "        for radio_signature in radio_consensus.split(b'|'):\n",
    "            n_bic_total += 1\n",
    "            percentage_consensus = (radio_counts[radio_consensus] /\n",
    "                                    total_classifications)\n",
    "            locations = []\n",
    "            for (_, x, y), (_, full, radio) in zip(pos_group, com_group):\n",
    "                if full == radio_consensus and radio == radio_signature:\n",
    "                    locations.append((x, y))\n",
    "            locations = numpy.array(locations)\n",
    "            locations = locations[~numpy.all(numpy.isnan(locations), axis=1)]\n",
    "            (x, y), success, reason = lowest_bic(locations)\n",
    "            \n",
    "            if not success and reason == 'mean':\n",
    "                n_bic_failure_mean += 1\n",
    "            elif not success and reason == 'low_redundancy':\n",
    "                n_bic_failure_tie += 1\n",
    "            elif not success and reason:\n",
    "                raise ValueError('Unknown failure reason: {}'.format(reason))\n",
    "            else:\n",
    "                n_bic_success += 1\n",
    "\n",
    "            if numpy.isnan(x) or numpy.isnan(y):\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC succeeded: 99.75%\n",
      "BIC failed (caught, mean used): 0.25%\n",
      "BIC failed (caught, best guess): 0.00%\n"
     ]
    }
   ],
   "source": [
    "print('BIC succeeded: {:.02%}'.format(n_bic_success / n_bic_total))\n",
    "print('BIC failed (caught, mean used): {:.02%}'.format(n_bic_failure_mean / n_bic_total))\n",
    "print('BIC failed (caught, best guess): {:.02%}'.format(n_bic_failure_tie / n_bic_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a *little* misleading. The BIC method, since it uses Gaussian fitting, may fit a single Gaussian to a few points if it's the best fit it can find, reducing it to the mean. Let's compare the labels to Norris et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from crowdastro.consensuses import lowest_bic_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(CROWDASTRO_H5_PATH, 'r') as f_h5:\n",
    "    # Copied from crowdastro.consensuses.\n",
    "    class_positions = f_h5['/atlas/cdfs/classification_positions']\n",
    "    class_combinations = f_h5['/atlas/cdfs/classification_combinations']\n",
    "    assert len(class_positions) == len(class_combinations)\n",
    "\n",
    "    logging.debug('Finding consensuses for %d classifications.',\n",
    "                  len(class_combinations))\n",
    "\n",
    "    # Pre-build the IR tree.\n",
    "    ir_coords = f_h5['/{}/cdfs/numeric'.format('swire')][:, :2]\n",
    "    ir_tree = sklearn.neighbors.KDTree(ir_coords)\n",
    "\n",
    "    cons_positions = []\n",
    "    cons_combinations = []\n",
    "\n",
    "    # Data integrity and assumptions checks.\n",
    "    assert numpy.array_equal(class_positions[:, 0],\n",
    "                             class_combinations['index'])\n",
    "    assert numpy.array_equal(class_positions[:, 0],\n",
    "                             sorted(class_positions[:, 0]))\n",
    "\n",
    "    pos_groups = itertools.groupby(class_positions, key=lambda z: z[0])\n",
    "    com_groups = itertools.groupby(class_combinations, key=lambda z: z['index'])\n",
    "\n",
    "    for (i, pos_group), (j, com_group) in zip(pos_groups, com_groups):\n",
    "        assert i == j\n",
    "\n",
    "        com_group = list(com_group)  # For multiple iterations.\n",
    "        pos_group = list(pos_group)\n",
    "        total_classifications = 0\n",
    "\n",
    "        # Find the radio consensus. Be wary when counting: If there are multiple\n",
    "        # AGNs identified in one subject, *that classification will appear\n",
    "        # multiple times*. I'm going to deal with this by dividing the weight of\n",
    "        # each classification by how many pipes it contains plus one.\n",
    "        radio_counts = {}  # Radio signature -> Count\n",
    "        for _, full_com, _ in com_group:\n",
    "            count = radio_counts.get(full_com, 0)\n",
    "            count += 1 / (full_com.count(b'|') + 1)\n",
    "            radio_counts[full_com] = count\n",
    "\n",
    "            total_classifications += 1 / (full_com.count(b'|') + 1)\n",
    "\n",
    "        for count in radio_counts.values():\n",
    "            # Despite the divisions, we should end up with integers overall.\n",
    "            assert numpy.isclose(round(count), count)\n",
    "        assert numpy.isclose(round(total_classifications),\n",
    "                             total_classifications)\n",
    "\n",
    "        radio_consensus = max(radio_counts, key=radio_counts.get)\n",
    "\n",
    "        # Find the location consensus. For each radio combination, run a\n",
    "        # location consensus function on the positions associated with that\n",
    "        # combination.\n",
    "        for radio_signature in radio_consensus.split(b'|'):\n",
    "            percentage_consensus = (radio_counts[radio_consensus] /\n",
    "                                    total_classifications)\n",
    "            locations = []\n",
    "            for (_, x, y), (_, full, radio) in zip(pos_group, com_group):\n",
    "                if full == radio_consensus and radio == radio_signature:\n",
    "                    locations.append((x, y))\n",
    "            locations = numpy.array(locations)\n",
    "            locations = locations[~numpy.all(numpy.isnan(locations), axis=1)]\n",
    "            (x, y), success = lowest_bic_gmm(locations)\n",
    "\n",
    "            if numpy.isnan(x) or numpy.isnan(y):\n",
    "                logging.debug('Skipping NaN PG-means output.')\n",
    "                continue\n",
    "\n",
    "            # Match the (x, y) position to an IR object.\n",
    "            dist, ind = ir_tree.query([(x, y)])\n",
    "\n",
    "            cons_positions.append((i, ind[0][0], success))\n",
    "            cons_combinations.append((i, radio_signature, percentage_consensus))\n",
    "\n",
    "    logging.debug('Found %d consensuses (before duplicate removal).',\n",
    "                  len(cons_positions))\n",
    "\n",
    "    # Remove duplicates. For training data, I don't really care if radio\n",
    "    # combinations overlap (though I need to care if I generate a catalogue!) so\n",
    "    # just take duplicated locations and pick the one with the highest radio\n",
    "    # consensus that has success.\n",
    "    cons_objects = {}  # Maps IR index to (ATLAS index, success,\n",
    "                       #                   percentage_consensus)\n",
    "    for (atlas_i, ir_i, success), (atlas_j, radio, percentage) in zip(\n",
    "            cons_positions, cons_combinations):\n",
    "        assert atlas_i == atlas_j\n",
    "\n",
    "        if ir_i not in cons_objects:\n",
    "            cons_objects[ir_i] = (atlas_i, success, percentage)\n",
    "            continue\n",
    "\n",
    "        if cons_objects[ir_i][1] and not success:\n",
    "            # Preference successful KDE/PG-means.\n",
    "            continue\n",
    "\n",
    "        if not cons_objects[ir_i][1] and success:\n",
    "            # Preference successful KDE/PG-means.\n",
    "            cons_objects[ir_i] = (atlas_i, success, percentage)\n",
    "            continue\n",
    "\n",
    "        # If we get this far, we have the same success state. Choose based on\n",
    "        # radio consensus.\n",
    "        if percentage > cons_objects[ir_i][2]:\n",
    "            cons_objects[ir_i] = (atlas_i, success, percentage)\n",
    "            continue\n",
    "\n",
    "    logging.debug('Found %d consensuses.', int(len(cons_objects)))\n",
    "\n",
    "    cons_objects = numpy.array([(atlas_i, ir_i, success, percentage)\n",
    "            for ir_i, (atlas_i, success, percentage)\n",
    "            in sorted(cons_objects.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Norris labels.\n",
    "with h5py.File(TRAINING_H5_PATH, 'r') as training_h5:\n",
    "    crowdsourced_labels = training_h5['labels'].value\n",
    "\n",
    "with h5py.File(CROWDASTRO_H5_PATH, 'r') as crowdastro_h5:\n",
    "    ir_names = crowdastro_h5['/swire/cdfs/string'].value\n",
    "    ir_positions = crowdastro_h5['/swire/cdfs/numeric'].value[:, :2]\n",
    "ir_tree = sklearn.neighbors.KDTree(ir_positions)\n",
    "\n",
    "with open(NORRIS_DAT_PATH, 'r') as norris_dat:\n",
    "    norris_coords = [r.strip().split('|') for r in norris_dat]\n",
    "\n",
    "norris_labels = numpy.zeros((len(ir_positions)))\n",
    "for ra, dec in norris_coords:\n",
    "    # Find a neighbour.\n",
    "    skycoord = SkyCoord(ra=ra, dec=dec, unit=('hourangle', 'deg'))\n",
    "    ra = skycoord.ra.degree\n",
    "    dec = skycoord.dec.degree\n",
    "    ((dist,),), ((ir,),) = ir_tree.query([(ra, dec)])\n",
    "    if dist < 0.1:\n",
    "        norris_labels[ir] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert radio labels into IR labels.\n",
    "bic_labels = numpy.zeros(norris_labels.shape)\n",
    "for _, ir_i, _, _ in cons_objects:\n",
    "    bic_labels[ir_i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(bic_labels, norris_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(crowdsourced_labels, norris_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
