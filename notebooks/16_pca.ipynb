{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "I'm curious as to how much information there is in each output of the CNN. I remember from ISML that PCA can work as a crude measure of this, so I'll grab some potential hosts, extract radio patches, run the CNN on them, and see how many principal components I need for each to recover most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "K:\\Languages\\Anaconda3\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import numpy\n",
    "import operator\n",
    "import sys\n",
    "\n",
    "import bson.objectid\n",
    "import keras.models\n",
    "import pandas\n",
    "import sklearn.decomposition\n",
    "\n",
    "sys.path.insert(1, '..')\n",
    "import crowdastro.data\n",
    "\n",
    "with open('../crowdastro-data/cnn_model_2.json', 'r') as f:\n",
    "    cnn = keras.models.model_from_json(f.read())\n",
    "\n",
    "cnn.load_weights('../crowdastro-data/cnn_weights_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>is_host</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'54b7f9ee0136916b75000002'</td>\n",
       "      <td>32.412434</td>\n",
       "      <td>65.519434</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'54b7f9ee0136916b75000002'</td>\n",
       "      <td>161.776174</td>\n",
       "      <td>130.278864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'54b7f9ee0136916b75000002'</td>\n",
       "      <td>101.349871</td>\n",
       "      <td>100.381640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'54b7f9ee0136916b75000002'</td>\n",
       "      <td>54.028687</td>\n",
       "      <td>186.482581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'54b7f9ee0136916b75000002'</td>\n",
       "      <td>102.648851</td>\n",
       "      <td>71.261272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subject_id           x           y is_host\n",
       "                             0           0           0       0\n",
       "0  b'54b7f9ee0136916b75000002'   32.412434   65.519434       0\n",
       "1  b'54b7f9ee0136916b75000002'  161.776174  130.278864       0\n",
       "2  b'54b7f9ee0136916b75000002'  101.349871  100.381640       1\n",
       "3  b'54b7f9ee0136916b75000002'   54.028687  186.482581       0\n",
       "4  b'54b7f9ee0136916b75000002'  102.648851   71.261272       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the potential hosts.\n",
    "with pandas.HDFStore('../crowdastro-data/training.h5') as store:\n",
    "    dataframe = store['metadata']  # New training data table is 'data'. Haven't run that script yet.\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_convolutional_output_1 = keras.backend.function([cnn.layers[0].input],\n",
    "                                                     [cnn.layers[1].get_output()])\n",
    "get_max_pooling_output_1 = keras.backend.function([cnn.layers[0].input],\n",
    "                                                   [cnn.layers[2].get_output()])\n",
    "get_convolutional_output_2 = keras.backend.function([cnn.layers[0].input],\n",
    "                                                     [cnn.layers[4].get_output()])\n",
    "get_max_pooling_output_2 = keras.backend.function([cnn.layers[0].input],\n",
    "                                                   [cnn.layers[5].get_output()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K:\\Languages\\Anaconda3\\lib\\site-packages\\astropy\\io\\fits\\util.py:578: UserWarning: Could not find appropriate MS Visual C Runtime library or library is corrupt/misconfigured; cannot determine whether your file object was opened in append mode.  Please consider using a file object opened in write mode instead.\n",
      "  'Could not find appropriate MS Visual C Runtime '\n",
      "K:\\Languages\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# Take the first thousand potential hosts, and get their associated radio patches.\n",
    "patches = []\n",
    "radius = 40\n",
    "padding = 150\n",
    "for potential_host in dataframe.to_records()[:1000]:  # Is there a better way to do this?\n",
    "    oid = bson.objectid.ObjectId(potential_host[1].tobytes().decode('ascii'))\n",
    "    subject = crowdastro.data.db.radio_subjects.find_one({'_id': oid})\n",
    "    radio = crowdastro.data.get_radio(subject, size='5x5')\n",
    "    x = potential_host[2]\n",
    "    y = potential_host[3]\n",
    "    patch = radio[x-radius+padding:x+radius+padding, y-radius+padding:y+radius+padding]\n",
    "    patches.append(patch)\n",
    "\n",
    "patches = numpy.array(patches).reshape(1000, 1, 80, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the patches through the CNN to get the intermediate outputs.\n",
    "conv_1 = get_convolutional_output_1([patches])[0]\n",
    "pool_1 = get_max_pooling_output_1([patches])[0]\n",
    "conv_2 = get_convolutional_output_2([patches])[0]\n",
    "pool_2 = get_max_pooling_output_2([patches])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape everything to be single images. We don't care about pixel order.\n",
    "conv_1 = conv_1.reshape(1000, -1)\n",
    "conv_2 = conv_2.reshape(1000, -1)\n",
    "pool_1 = pool_1.reshape(1000, -1)\n",
    "pool_2 = pool_2.reshape(1000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv 1 dimension: 161312\n",
      "pool 1 dimension: 6272\n",
      "conv 2 dimension: 800\n",
      "pool 2 dimension: 32\n"
     ]
    }
   ],
   "source": [
    "# How many dimensions does each layer have?\n",
    "print('conv 1 dimension:', functools.reduce(operator.mul, conv_1.shape[1:]))\n",
    "print('pool 1 dimension:', functools.reduce(operator.mul, pool_1.shape[1:]))\n",
    "print('conv 2 dimension:', functools.reduce(operator.mul, conv_2.shape[1:]))\n",
    "print('pool 2 dimension:', functools.reduce(operator.mul, pool_2.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run PCA on each of these.\n",
    "pca_conv_1 = sklearn.decomposition.PCA(n_components=0.999)\n",
    "pca_conv_1.fit(conv_1)\n",
    "pca_conv_1.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_pool_1 = sklearn.decomposition.PCA(n_components=0.999)\n",
    "pca_pool_1.fit(pool_1)\n",
    "pca_pool_1.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_conv_2 = sklearn.decomposition.PCA(n_components=0.999)\n",
    "pca_conv_2.fit(conv_2)\n",
    "pca_conv_2.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_pool_2 = sklearn.decomposition.PCA(n_components=0.999)\n",
    "pca_pool_2.fit(pool_2)\n",
    "pca_pool_2.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to recover 99.9% of the variance, we need to have less than 200 features. However, our convolutional neural network outputs way more than this at each layer (and a lot less at the last layer). Maybe we can take the second convolutional output and run PCA on it as a form of compression? I'd rather have 155 dimensions than 800.\n",
    "\n",
    "With that in mind, I'll run the PCA again on just the second convolutional output and freeze it. Unfortunately, it seems sklearn doesn't support anything except pickle &mdash; which I definitely don't want to use &mdash; so I'm going to have to serialise it myself. I'll use h5py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 800)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_conv_2.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('../crowdastro-data/pca.h5', 'w') as f:\n",
    "    dset = f.create_dataset(\"conv_2\", (155, 800), dtype=float)\n",
    "    dset[:] = pca_conv_2.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
