%!TEX root=thesis.tex
% Unsorted writing.

\chapter{Imagine There's No Chapters}
\label{cha:imagine}

\section{Papers Read}
\label{sec:papers}
  
  \begin{itemize}
    \item \citet{banfield15}
    \item \citet{yan10}
    \item \citet{yan11}
    \item \citet{dasgupta11}
    \item \citet{mozafari12} (re-reading in progress)
    \item \citet{fan15} (probably need to re-read)
    \item \citet{freund97} (need to re-read)
  \end{itemize}

\section{The Radio Galaxy Zoo}
\label{sec:rgz}

    Many galaxies contain a supermassive black hole in their centre. These
    black holes draw in surrounding matter, and may produce jets of matter as
    they do. These jets emit radio waves \todo{Or X-rays, I think, which are
    then redshifted to radio. Confirm and rewrite.} which can then be detected
    by radio telescopes. As black holes cannot be observed directly, this is
    the only way to identify black holes in distant galaxies. A radio-loud
    black hole in the centre of a galaxy is called an active galactic nucleus
    (AGN). \todo{I'm having some trouble finding information on AGNs,
    especially on their definitions. Find something concrete.} Large radio
    surveys such as Faint Images of the Radio Sky at Twenty-Centimeters (FIRST)
    \citep{white97, becker95} and the Australian Telescope Large Area Survey
    (ATLAS) \citep{franzen15} have found many sources of radio emissions, and
    these sources are dominated by AGNs \citep{banfield15}.

\section{Radio Galaxy Zoo Consensus Labels}

\section{Radio Cross-identification}
\label{sec:cross-identification}

    Each radio object has some associated infrared object called the host
    galaxy. The cross-identification task is to find the host galaxy given the
    radio object.
    
    For modelling the distribution, I have chosen to use logistic regression,
    i.e.
    \begin{equation}
        \label{eq:logistic-regression-cross-identification}
        p(y(x) = 1 \mid x, r) = \vec w \cdot \vec \phi(x, r)
    \end{equation}
    where $\vec \phi$ is a feature space mapping dependent on a galaxy and a
    radio object. The features should represent the galaxy in some way, so I
    have chosen the following feature space:
    \begin{equation}
        \label{eq:galaxy-features}
        \vec \phi(x, r) = \begin{pmatrix}
            \vec{\mbox{flux}}(x)\\
            \mbox{dist}(x, r)\\
            \vec{\mbox{cnn}}(\mbox{radio}(x))
        \end{pmatrix}
    \end{equation}
    $\vec{\mbox{flux}}(x)$ is a vector of infrared flux measurements of $x$,
    which can be obtained from the infrared survey catalogue. $\mbox{dist}(x,
    r)$ is the Euclidean distance across the sky between the centre of the $x$
    and the centre of $r$. $\vec{\mbox{cnn}}(m)$ is the output of the
    convolutional neural network on input image $m$, and $\mbox{radio}(x)$ is a
    $0.8' \times 0.8'$ image of the radio sky centred on $x$.

\section{Training Data}
\label{sec:training-data}
  
  The Crowdastro dataset is a set of training data for the binary
  classification problem described in Section \ref{sec:cross-identification}.
  The dataset contains features and labels for all objects detected in the WISE
  infrared survey. The prediction task is to predict the label of an object
  given its features.

  The features are not scaled and have not undergone any feature extraction
  process. They are the raw fluxes, distances, and radio images described in
  Section \ref{sec:cross-identification}.

  The labels are based on the consensus locations from the Radio Galaxy Zoo,
  matched to the nearest WISE object. WISE objects matched to a consensus
  location have the label $1$, and all other objects have the label $0$.
  Consensuses are found as described in Section \ref{sec:consensuses}, with
  consensus location decided by fitting a Gaussian mixture model. The number of
  Gaussians is found by a grid search minimising the Baysian information
  criterion.


\section{Yan Derivation}
\label{sec:yan-derivation}
  In this section, I elaborate on the derivation of the expectation-maximisation formulae from \citet{yan10}. Notation etc. is taken from \citet{yan10}. I only consider the case of binary labels.

  \subsection{Formulation}

      We have $N$ data points $\{\vec x_1, \dots, \vec x_N\}$, where $\vec x_i \in \mathbb{R}^D$. We also have a set of labels $\{y_1^{(1)}, \dots, y_n^{(1)},$ $\dots, y_1^{(T)}, \dots, y_N^{(T)}\}$, where $y_i^{(t)} \in \mathbb{Z}_2$ is the (potentially incorrect) binary label assigned to $\vec x_i$ by annotator $t$. We want to train a classifier to predict labels of new data points, we want to estimate the groundtruth labels $\{z_1, \dots, z_N\}$ where $z_i \in \mathbb{Z}_2$, and we want to model the quality of each annotator's labels. Let $\vec x$, $y$, and $z$ be random variables representing data points, labels, and groundtruths, respectively. The classification task is then to model $p(z \mid \vec x)$.
      Define matrices to represent the data:
      \begin{align*}
          X &= [\vec x_1^T; \dots; \vec x_N^T] \in \mathbb{R}^{N \times D}\\
          Y &= [y_1^{(1)}, \dots, y_1^{(T)}; \dots; y_N^{(1)}, \dots, y_N^{(T)}] \in \mathbb{Z}_2^{N \times T}\\
          Z &= (z_1, \dots, z_N) \in \mathbb{Z}_2^N
      \end{align*}
      We assume that annotator labels depend on both the data point and the groundtruth, that annotator labels have annotator-dependent noise, and that annotator labels follow a Bernoulli distribution:
      \begin{align*}
          p(y_i^{(t)} \mid \vec x_i, z_i, \vec w_t, \gamma_t) &= (1 - \eta_t(\vec x_i \mid \vec w_t, \gamma_t))^{|y_i^{(t)} - z_i|} \eta_t(\vec x_i \mid \vec w_t, \gamma_t)^{1 - |y_i^{(t)} - z_i|}\\
          \eta_t(\vec x_i \mid \vec w_t, \gamma_t) &= \sigma(\vec w_t^T \vec x_i - \gamma_t)
      \end{align*}
      We use logistic regression to model the posterior distribution:
      \[
          p(z_i = 1 \mid \vec x_i, \alpha, \beta) = \sigma(\vec \alpha^T \vec x_i + \beta)
      \]
      The parameters of the model are $\vec \theta = \{\vec \alpha, \beta, \vec w_1, \dots, \vec w_T, \gamma_1, \dots, \gamma_T\}$.

  \subsection{Expectation-Maximisation}

      We can find optimum values of the parameters by maximising the log-likelihood $p(Y \mid X, \vec \theta)$, i.e.
      \begin{align*}
          \vec \theta^\star &= \underset{\vec \theta}{\mbox{argmax}} \sum_{i = 1}^N \sum_{t = 1}^T \log p(y_i^{(t)} \mid \vec x_i, \vec \theta)\\
              &= \underset{\vec \theta}{\mbox{argmax}} \sum_{i = 1}^N \sum_{t = 1}^T \log \sum_{z_i = 0}^1 p(y_i^{(t)}, z_i \mid \vec x_i, \vec \theta)
      \end{align*}
      noting that we assume independence between labels of different data points and labels from different annotators. Since $z_i$ are latent variables, we must use expectation-maximisation.\\

      \subsubsection{Expectation}

        For the expectation step, we want to evaluate $p(z_i \mid \vec x_i, y_i^{(1)}, \dots, y_i^{(T)}, \vec \theta)$ for $i = 1, \dots, N$. We can write this in terms of the parameters:
        \begin{align*}
            p(z_i \mid \vec x_i, y_i^{(1)}, \dots, y_i^{(T)}, \vec \theta) &= \frac{1}{A_i} p(z_i, y_i^{(1)}, \dots, y_i^{(T)} \mid \vec x_i, \vec \theta)\\
                &= \frac{1}{A_i} \prod_{t = 1}^T p(z_i, y_i^{(t)} \mid \vec x_i, \vec \theta)\\
                &= \frac{1}{A_i} \prod_{t = 1}^T p(y_i^{(t)} \mid \vec x_i, z_i, \vec \theta) p(z_i \mid \vec x_i, \vec \theta)\\
                &= \frac{1}{A_i} \prod_{t = 1}^T p(y_i^{(t)} \mid \vec x_i, z_i, \vec w_t, \gamma_t) p(z_i \mid \vec x_i, \vec \alpha, \beta)
        \end{align*}
        $A_i$ is a normalisation term given by
        \[
            A_i = \sum_{z_i = 0}^1 p(z_i, y_i^{(1)}, \dots, y_i^{(T)} \mid \vec x_i, \vec \theta).
        \]
        To simplify notation, let $\tilde p(z_i) = p(z_i \mid \vec x_i, y_i^{(1)}, \dots, y_i^{(T)}, \vec \theta)$.

      \subsubsection{Maximisation}

        For the maximisation step, we want to set
        \[
            \vec \theta^{\text{new}} = \underset{\vec \theta^{\text{new}}}{\mbox{argmax}} \sum_{i = 1}^N Q_i(\vec \theta^{\text{new}}, \vec \theta)
        \]
        where
        \[
            Q_i(\vec \theta^{\text{new}}, \vec \theta) = \sum_{z_i = 0}^1 p(z_i \mid \vec x_i, y_i^{(1)}, \dots, y_i^{(T)}, \vec \theta) \log p(\vec x_i, y_i^{(1)}, \dots, y_i^{(T)}, z_i \mid \vec \theta^{\text{new}}).
        \]
        Once again, we need to write this in terms of the parameters.
        \begin{align*}
            Q_i(\vec \theta^{\text{new}}, \vec \theta) &= \sum_{z_i = 0}^1 \tilde p(z_i) \log p(\vec x_i, y_i^{(1)}, \dots, y_i^{(T)}, z_i \mid \vec \theta^{\text{new}})\\
                &= \sum_{z_i = 0}^1 \sum_{t = 1}^T \tilde p(z_i) \log p(\vec x_i, y_i^{(t)}, z_i \mid \vec \theta^{\text{new}})\\
                &= \sum_{z_i = 0}^1 \sum_{t = 1}^T \tilde p(z_i) \log (p(y_i^{(t)}, z_i \mid \vec x_i, \vec \theta^{\text{new}}) p(\vec x_i \mid \vec \theta^{\text{new}}))\\
                &=  T \log p(\vec x_i \mid \vec \theta^{\text{new}}) + \sum_{z_i = 0}^1 \sum_{t = 1}^T \tilde p(z_i) \log p(y_i^{(t)}, z_i \mid \vec x_i, \vec \theta^{\text{new}})\\
                \begin{split}&= T \log p(\vec x_i \mid \vec \theta^{\text{new}}) + \\
                             &\quad\quad \sum_{z_i = 0}^1 \sum_{t = 1}^T \tilde p(z_i) (\log p(y_i^{(t)}\mid \vec x_i, z_i, \vec \theta^{\text{new}}) + \log p(z_i \mid \vec x_i, \vec \theta^{\text{new}}))
                \end{split}\\
                \begin{split}&= T \log p(\vec x_i \mid \vec \theta^{\text{new}}) + \\
                             &\quad\quad \sum_{z_i = 0}^1 \sum_{t = 1}^T \tilde p(z_i) (\log p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t^{\text{new}}, \gamma_t^{\text{new}}) + \log p(z_i \mid \vec x_i, \vec \alpha^{\text{new}}, \beta^{\text{new}}))
                \end{split}
        \end{align*}
        Then the maximisation step is
        \[
            \vec \theta^{\text{new}} = \underset{\vec \theta^{\text{new}}}{\mbox{argmax}} \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{t = 1}^T \tilde p(z_i) (\log p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t^{\text{new}}, \gamma_t^{\text{new}}) + \log p(z_i \mid \vec x_i, \vec \alpha^{\text{new}}, \beta^{\text{new}}))
        \]
        noting that $T \log p(\vec x_i \mid \vec \theta^{\text{new}})$ is the same for all $\vec \theta^{\text{new}}$ as $x_i$ is observed. To simplify notation, let
        \[
            f(\vec \theta) = \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{t = 1}^T \tilde p(z_i) (\log p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t) + \log p(z_i \mid \vec x_i, \vec \alpha, \beta)).
        \]
        where $\tilde p(z_i)$ is evaluated using the old value of $\vec \theta$.

    \newpage
    \subsection{Gradients of the Optimisation Target}

        In this section, I derive the gradients of $f$ with respect to the parameters $\vec \theta$.

        First, we differentiate with respect to $\vec \alpha$.

        \begin{align*}
            \nabla_{\vec \alpha} f(\vec \theta) &= T \sum_{i = 1}^N \sum_{z_i = 0}^1 \nabla_{\vec \alpha} (\tilde p(z_i) \log p(z_i \mid \vec x_i, \vec \alpha, \beta))\\
                &= T \sum_{i = 1}^N \sum_{z_i = 0}^1 \tilde p(z_i) \nabla_{\vec \alpha} \log p(z_i \mid \vec x_i, \vec \alpha, \beta)\\
                &= T \sum_{i = 1}^N \tilde p(z_i = 1) \nabla_{\vec \alpha} \log \sigma(\vec \alpha^T \vec x_i + \beta) + \tilde p(z_i = 0) \nabla_{\vec \alpha} \log (1 - \sigma(\vec \alpha^T \vec x_i + \beta))\\
                &= T \sum_{i = 1}^N \frac{\tilde p(z_i = 1)}{\sigma(\vec \alpha^T \vec x_i + \beta)} \nabla_{\vec \alpha} \sigma(\vec \alpha^T \vec x_i + \beta) - \frac{\tilde p(z_i = 0)}{1 - \sigma(\vec \alpha^T \vec x_i + \beta)} \nabla_{\vec \alpha} \sigma(\vec \alpha^T \vec x_i + \beta)\\
                &= T \sum_{i = 1}^N \left(\tilde p(z_i = 1) (1 - \sigma(\vec \alpha^T \vec x_i + \beta)) - \tilde p(z_i = 0) \sigma(\vec \alpha^T \vec x_i + \beta)\right) \vec x_i\\
                &= T \sum_{i = 1}^N \left(\tilde p(z_i = 1) (1 - \sigma(\vec \alpha^T \vec x_i + \beta)) - (1 - \tilde p(z_i = 1)) \sigma(\vec \alpha^T \vec x_i + \beta)\right) \vec x_i\\
                &= T \sum_{i = 1}^N \left(\tilde p(z_i = 1) - \sigma(\vec \alpha^T \vec x_i + \beta)\right) \vec x_i\\
        \end{align*}

        Following similar logic, we also obtain the gradient with respect to $\beta$:
        \[
            \frac{\partial f}{\partial \beta} f(\vec \theta) = T \sum_{i = 1}^N \tilde p(z_i = 1) - \sigma(\vec \alpha^T \vec x_i + \beta)
        \]

        We now differentiate with respect to $\vec w_t$.

        \begin{align*}
            \nabla_{\vec w_t} f(\vec \theta) &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{t = 1}^T \tilde p(z_i) \nabla_{\vec w_t} \log p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t)\\
                &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{t = 1}^T \frac{\tilde p(z_i)}{p(y_i^{(t)} \mid \vec x_i, z_i, \vec w_t, \gamma_t)} \nabla_{\vec w_t} p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t)\\
                &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{t = 1}^T \frac{\tilde p(z_i)}{p(y_i^{(t)} \mid \vec x_i, z_i, \vec w_t, \gamma_t)} \frac{\partial}{\partial \eta_t} p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t) \nabla_{\vec w_t} \eta_t(\vec x_i \mid \vec w_t, \gamma_t)\\
                &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{t = 1}^T - \tilde p(z_i) \frac{(1-\eta_t)^{\abs{y_i^{(t)}-z_i}-1} \eta_t^{-\abs{y_i^{(t)}-z_i}} \left(\eta_t+\abs{y_i^{(t)}-z_i}-1\right)}{p(y_i^{(t)} \mid \vec x_i, z_i, \vec w_t, \gamma_t)} \nabla_{\vec w_t} \eta_t(\vec x_i \mid \vec w_t, \gamma_t)\\
                &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{t = 1}^T - \tilde p(z_i) \frac{\eta_t(\vec x_i \mid \vec w_t, \gamma_t)+\abs{y_i^{(t)}-z_i}-1}{(1-\eta_t(\vec x_i \mid \vec w_t, \gamma_t)) \eta_t(\vec x_i \mid \vec w_t, \gamma_t)} \nabla_{\vec w_t} \eta_t(\vec x_i \mid \vec w_t, \gamma_t)\\
                &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{t = 1}^T - \tilde p(z_i) \left(\eta_t(\vec x_i \mid \vec w_t, \gamma_t)+\abs{y_i^{(t)}-z_i}-1\right)\vec x_i\\
                &= \sum_{i = 1}^N \sum_{t = 1}^T - \tilde p(z_i = 1) \left(\eta_t(\vec x_i \mid \vec w_t, \gamma_t) - y_i^{(t)}\right)\vec x_i - \tilde p(z_i = 0) \left(\eta_t(\vec x_i \mid \vec w_t, \gamma_t) + y_i^{(t)} - 1\right)\vec x_i\\
                &= \sum_{i = 1}^N \sum_{t = 1}^T \left(
                    2 \tilde p(z_i = 1) y_i^{(t)}
                    - \eta_t(\vec x_i \mid \vec w_t, \gamma_t)
                    - y_i^{(t)}
                    + 1
                    - \tilde p(z_i = 1)
                \right)\vec x_i\\
                &= \sum_{i = 1}^N \sum_{t = 1}^T \left(
                    2 \tilde p(z_i = 1) y_i^{(t)}
                    - \eta_t(\vec x_i \mid \vec w_t, \gamma_t)
                    - y_i^{(t)}
                    + \tilde p(z_i = 0)
                \right)\vec x_i\\
        \end{align*}

        Similarly, the gradient with respect to $\gamma_t$ is
        \[
            \nabla_{\gamma_t} f(\vec \theta) = \sum_{i = 1}^N \sum_{t = 1}^T
                    2 \tilde p(z_i = 1) y_i^{(t)}
                    - \eta_t(\vec x_i \mid \vec w_t, \gamma_t)
                    - y_i^{(t)}
                    + \tilde p(z_i = 0)
        \]


        % \subsubsection{$\frac{\partial f}{\partial \beta}(\vec \theta)$}
  
        %     The derivative is mostly identical to the derivative for $\nabla_{\vec \alpha} f(\vec \theta)$, but with $\frac{\partial}{\partial \beta} (\vec \alpha^T \vec x_i + \beta) = 1$.
  
        %     \[
        %         \frac{\partial f}{\partial \beta}(\vec \theta) = T \sum_{i = 1}^N (\tilde p(z_i = 1) - \tilde p(z_i = 0)) (1 - \sigma(\vec \alpha^T \vec x_i + \beta))\\
        %     \]

        % \subsubsection{$\nabla_{\vec w_t} f(\vec \theta)$}

        %     \begin{align*}
        %         \nabla_{\vec w_t} f(\vec \theta) &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \sum_{s = 1}^T \nabla_{\vec w_t} (\tilde p(z_i) \log p(y_i^{(s)}\mid \vec x_i, z_i, \vec w_s, \gamma_s))\\
        %             &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \nabla_{\vec w_t} (\tilde p(z_i) \log p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t))\\
        %             &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \tilde p(z_i) \nabla_{\vec w_t} \log p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t)\\
        %             &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \tilde p(z_i) \frac{\nabla_{\vec w_t} p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t)}{p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t)}\\
        %             &= \sum_{i = 1}^N \sum_{z_i = 0}^1 \tilde p(z_i) \frac{\nabla_{\eta_t} p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t) \nabla_{\vec w_t} \eta_t(x_i \mid \vec w_t, \gamma_t)}{p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t)}\\
        %             \begin{split}&= \sum_{i = 1}^N \sum_{z_i = 0}^1 \tilde p(z_i) \frac{1}{p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t)}\\
        %                 &\times (1 - {\eta_t(x_i \mid \vec w_t, \gamma_t)})^{|y_i^{(t)} - z_i|} - {|y_i^{(t)} - z_i|}(1 - {\eta_t(x_i \mid \vec w_t, \gamma_t)})^{{|y_i^{(t)} - z_i|} - 1})\\
        %                 &\times {\eta_t(x_i \mid \vec w_t, \gamma_t)}^{-{|y_i^{(t)} - z_i|}}\nabla_{\vec w_t} \eta_t(x_i \mid \vec w_t, \gamma_t)\\
        %             \end{split}\\
        %             \begin{split}&= \sum_{i = 1}^N \sum_{z_i = 0}^1 \tilde p(z_i) \frac{1}{p(y_i^{(t)}\mid \vec x_i, z_i, \vec w_t, \gamma_t)}\\
        %                 &\times (1 - {\eta_t(x_i \mid \vec w_t, \gamma_t)})^{|y_i^{(t)} - z_i|} - {|y_i^{(t)} - z_i|}(1 - {\eta_t(x_i \mid \vec w_t, \gamma_t)})^{{|y_i^{(t)} - z_i|} - 1})\\
        %                 &\times {\eta_t(x_i \mid \vec w_t, \gamma_t)}^{-{|y_i^{(t)} - z_i|}}\nabla_{\vec w_t} \eta_t(x_i \mid \vec w_t, \gamma_t)\\
        %             \end{split}\\
        %     \end{align*}
