%!TeX program=xelatex
\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}

\usepackage{fontspec}
\setmainfont{Latin Modern Roman}

\renewcommand{\vec}{\boldsymbol}
\newcommand{\vectilde}[1]{\tilde{\boldsymbol{#1}}}

\begin{document}
    \title{Active Learning with Crowdsourced Labels}
    \author{Matthew Alger \\ \emph{The Australian National University}}
    \maketitle

    Crowdsourcing provides an active learning domain where many standard active learning assumptions are broken: There is no longer just one labeller, labellers may be non-expert, labellers may not be independent, labellers' accuracy may differ depending on the examples presented, and different labellers may have different accuracies.

    Yan et al.\cite{yan10} introduce a probabilistic model of the crowdsourced active learning problem. Denote examples as $\vec x_1, \dots, \vec x_N$ with $\vec x_i \in \mathbb{R}^D$, true (unknown) labels as $z_1, \dots, z_N$, and labels given by the labeller $t$ as $y^t_1, \dots, y^t_N$. Not all $y^t_i$ are observed and generally no $z_i$ are observed. Denote the $N \times D$ matrix of all examples as $X$, the $N \times 1$ matrix of all true labels as $Z$, and the $N \times T$ matrix of all labeller-generated labels as $Y$ (where $T$ is the number of labellers). Then
    \[
        p(Y, Z \mid X) = \prod_i p(z_i \mid \vec x_i) \prod_{t = 1}^T p(y^t_i \mid \vec x_i, z_i).
    \]
    This model makes the label $y^t_i$ dependent on not only the true label $z_i$ but also the specific example $\vec x_i$. As such, it addresses the problem of labellers' accuracy differing depending on the examples presented as well as differing from each other in general. $p(z_i \mid \vec x_i)$ models the likelihood; Yan et al. use logistic regression:
    \[
        p(z_i \mid \vec x_i) = (1 + \exp(- \vec a \cdot \vec x_i - \beta))^{-1}
    \]
    $p(y^t_i \mid \vec x_i, z_i)$ models the labeller; for binary classification, Yan et al. use a Bernoulli model with
    \[
        p(y^t_i \mid \vec x_i, z_i) = (1 - \eta_t(\vec x_i))^{|y^t_i - z_i|} \eta_t(\vec x_i)^{1 - |y^t_i - z_i|}
    \]
    where $\eta_t$ is a logistic function with parameters $\vec w$ and $\gamma$. This model can be trained with expectation maximisation.

    Yan et al.\cite{yan11} use this model to select an unlabelled example, and then to select a labeller to show the example to. First, they select an unlabelled example using uncertainty sampling\cite{lewis94}. This amounts to finding $\vectilde x$ such that
    \[
        \vectilde x = \min_{\vec x_i} \left(\frac{1}{2} - p(z_i \mid \vec x_i)\right)^2
    \]
    Under logistic regression, this defines a hyperplane of $\vec x$ that we may select to label:
    \[
        \vec \alpha \cdot \vec x + \beta = 0
    \]
    We then want to choose a point on this hyperplane and a labeller such that the labeller has minimum error --- i.e., we want to find $\vectilde x$ and $\tilde t$ such that
    \[
        \vectilde x, \tilde t = \min_{\vectilde x, \tilde t} \eta_t(\vectilde x)
    \]
    Choosing both examples and labellers in this way results in improved performance over just choosing the examples (and dealing with label noise by majority vote) and just choosing the labeller (and randomly sampling examples).

    Mozafari et al.\cite{mozafari12} make use of two nonparametric bootstrap methods to decide which examples to present to the crowd in a binary classification task. For a classifier $\theta$ trained on a set of training data $L$, and a data point $u \in L$, we want to find the uncertainty in $\theta_L(u)$. If we had many different $L$ drawn from the same distribution, then by evaluating $\theta_L(u)$ for all of these different $L$, we could measure properties of the distribution of $\theta(u)$, such as the variance. However, we generally can't draw more $L$ from the original distribution. This is where a nonparametric bootstrap method comes in: Consider $L$ as a proxy for the original distribution $L$ was drawn from, and draw sets of independent and identically distributed samples from $L$ with replacement. These sets are called ``bootstrap replicates''. Bootstrap replicates should have the same cardinality as $L$. Training a classifier on each of the bootstrap replicates then allows the distribution of $\theta(u)$ to be approximated. For bootstrapping to work, it is sufficient that $\theta$ is smooth. By using bootstrapping, the two methods proposed by Mozafari et al. work on most classifiers (most importantly, non-probabilistic classifiers) and treat these classifiers as ``black boxes'' (i.e., the internal state of the classifier is not used). According to the paper, bootstrapping gives less biased estimates of uncertainty than entropy- or margin-based approaches.

    The two methods are the Uncertainty method and the MinExpError method. Both methods generate scores for each data point $u$. Uncertainty is a modification of uncertainty sampling\cite{cohn94}, and is given as
    \[
        \mbox{Uncertainty}(u) = \mbox{var}(\theta_L(u))
    \]
    where the variance is found by bootstrapping. MinExpError scores data points higher the more that their labelling affects the model. Let $l$ be the label of $u$ found by $\theta_L$. Train $k$ classifiers on $k$ bootstrap replicates and use these to generate $k$ labels $l_1, \dots, l_k$ for $u$. Then, use these to estimate the probability $l$ is incorrect as
    \[
        \hat p(u) = \frac{\sum_{i = 1}^k 1(l_i = l)}{k}
    \]
    where $1(c)$ is $1$ when $c$ is true and is $0$ otherwise. The MinExpError score is then given by
    \[
        \mbox{MinExpError}(u) = \hat p(u) \hat e_{\text{right}} + (1 - \hat p(u)) \hat e_{\text{wrong}}
    \]
    where $e_\text{right}$ is the error of the classifier trained on $L$ and $(u, l)$ and $e_\text{wrong}$ is the error of the classifier trained on $L$ and $(u, 1 - l)$.

    Mozafari et al. also propose a method of dealing with crowd label noise, by dynamically finding the level of redundancy needed for different subsets of unlabelled examples. They observe that there is a labelling redundancy threshold above which there is no benefit in getting more labels for the same data point, and that different classes can have dramatically different accuracies. The partitioning based allocation (PBA) approach presented in the paper estimates the probability of correctly classifying a class, and then solves an integer linear program to find the level of redundancy needed. I have not reproduced the algorithm here.

    The final section of Mozafari et al. deals with stopping criteria for active learning on crowds, but I will ignore this here.

    \bibliographystyle{unsrt}
    \bibliography{papers}

\end{document}
